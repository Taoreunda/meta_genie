#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì œëª© ê¸°ë°˜ Abstract ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸

meta_article_data.csvì˜ Titleì„ ê¸°ì¤€ìœ¼ë¡œ abstract-depression-set.txtì—ì„œ 
í•´ë‹¹í•˜ëŠ” abstractë¥¼ ì°¾ì•„ì„œ ë§¤ì¹­í•©ë‹ˆë‹¤.
"""

import csv
import os
import re
from datetime import datetime
from difflib import SequenceMatcher

def normalize_title(title):
    """ì œëª© ì •ê·œí™” í•¨ìˆ˜"""
    if not title:
        return ""
    
    # ì†Œë¬¸ì ë³€í™˜
    normalized = title.lower()
    
    # íŠ¹ìˆ˜ë¬¸ì ë° êµ¬ë‘ì  ì œê±°/ì •ê·œí™”
    normalized = re.sub(r'[^\w\s]', ' ', normalized)
    
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€í™˜
    normalized = re.sub(r'\s+', ' ', normalized)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    normalized = normalized.strip()
    
    return normalized

def extract_keywords(title, min_length=4):
    """ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    if not title:
        return []
    
    # ì¼ë°˜ì ì¸ stopwords ì œê±°
    stopwords = {
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
        'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during',
        'before', 'after', 'above', 'below', 'between', 'among', 'is', 'are',
        'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does',
        'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can',
        'using', 'based', 'study', 'research', 'analysis', 'trial', 'systematic',
        'review', 'meta'
    }
    
    normalized = normalize_title(title)
    words = normalized.split()
    
    # stopwordsê°€ ì•„ë‹ˆê³  ìµœì†Œ ê¸¸ì´ ì´ìƒì¸ ë‹¨ì–´ë“¤ë§Œ ì¶”ì¶œ
    keywords = [word for word in words if word not in stopwords and len(word) >= min_length]
    
    return keywords

def calculate_similarity(title1, title2):
    """ë‘ ì œëª© ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
    # ì •ê·œí™”ëœ ì œëª©ìœ¼ë¡œ ì§ì ‘ ë¹„êµ
    norm1 = normalize_title(title1)
    norm2 = normalize_title(title2)
    
    # ì™„ì „ ì¼ì¹˜
    if norm1 == norm2:
        return 1.0
    
    # ì‹œí€€ìŠ¤ ë§¤ì²˜ë¡œ ìœ ì‚¬ë„ ê³„ì‚°
    similarity = SequenceMatcher(None, norm1, norm2).ratio()
    
    # í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ë„ ê³„ì‚°
    keywords1 = set(extract_keywords(title1))
    keywords2 = set(extract_keywords(title2))
    
    if keywords1 and keywords2:
        keyword_similarity = len(keywords1.intersection(keywords2)) / len(keywords1.union(keywords2))
        # ì „ì²´ ìœ ì‚¬ë„ì™€ í‚¤ì›Œë“œ ìœ ì‚¬ë„ì˜ ê°€ì¤‘ í‰ê· 
        similarity = (similarity * 0.6) + (keyword_similarity * 0.4)
    
    return similarity

def parse_single_record_v2(record_text, number):
    """
    extract_abstract_v2.pyì˜ íŒŒì‹± ë¡œì§ ì¬ì‚¬ìš©
    ë‹¨ì¼ ë ˆì½”ë“œ íŒŒì‹±
    """
    lines = record_text.split('\n')
    
    # ì´ˆê¸°í™”
    title = None
    abstract = None
    doi = None
    
    # 1. DOI ì¶”ì¶œ - ì²« ì¤„ê³¼ ë‘ ë²ˆì§¸ ì¤„ì—ì„œ ì°¾ê¸°
    first_line = lines[0] if lines else ""
    doi_match = re.search(r'doi:\s*(10\.\d{4,9}/[^\s]+)', first_line, re.IGNORECASE)
    if doi_match:
        doi = doi_match.group(1).strip().rstrip('.')
    else:
        # ë‘ ë²ˆì§¸ ì¤„ì—ì„œ DOI ì°¾ê¸°
        if len(lines) > 1 and 'doi:' in first_line.lower():
            second_line = lines[1].strip()
            if re.match(r'^10\.\d{4,9}/[^\s]+\.?$', second_line):
                doi = second_line.rstrip('.')
    
    # 2. ì œëª© ì°¾ê¸° - Author information ì´ì „ê¹Œì§€
    author_info_idx = -1
    for idx, line in enumerate(lines):
        if 'Author information:' in line:
            author_info_idx = idx
            break
    
    if author_info_idx != -1:
        # Author information ì´ì „ê¹Œì§€ì˜ í…ìŠ¤íŠ¸ì—ì„œ ì œëª© ì¶”ì¶œ
        title_part = record_text[:record_text.find('Author information:')]
        temp_lines = title_part.split('\n')
        title_lines = []
        skip_empty_lines = 0
        
        for i, line in enumerate(temp_lines[1:], 1):
            line = line.strip()
            
            # ê±´ë„ˆë›¸ ì¤„ë“¤ (DOI, ë…„ë„, Epub ë“±)
            if (re.search(r'^10\.\d{4,9}/[^\s]+\.?$', line) or  # DOI íŒ¨í„´
                re.search(r'10\.\d{4,9}/[^\s]+\.?\s*Epub', line) or  # DOI + Epub íŒ¨í„´
                re.search(r'^\d{4}|^Epub|^pp\.', line) or  # ë…„ë„, Epub, í˜ì´ì§€
                len(line) < 10):  # ì§§ì€ ì¤„
                continue
                
            # ë¹ˆ ì¤„ ì²˜ë¦¬
            if not line:
                skip_empty_lines += 1
                if skip_empty_lines >= 2:
                    break
                continue
            else:
                skip_empty_lines = 0
            
            # ì €ìëª… íŒ¨í„´ ê°ì§€
            if re.search(r'[A-Z][a-z]+\s+[A-Z]{2,}\(\d+\)', line):
                break
            
            # ì €ì ì°¸ì¡° ë²ˆí˜¸ íŒ¨í„´ ê°ì§€ ((1), (2) ë“±)
            if re.search(r'\(\d+\)', line):
                break
            
            # ì œëª©ì— ì¶”ê°€
            title_lines.append(line)
        
        if title_lines:
            title = ' '.join(title_lines).strip()
            # í›„ì²˜ë¦¬: ì œëª© ëì˜ ë§ˆì¹¨í‘œ ì œê±°
            if title and title.endswith('.'):
                title = title.rstrip('.')
    
    # 3. Abstract ì¶”ì¶œ - Author information ì´í›„
    abstract_lines = []
    if author_info_idx != -1:
        abstract_started = False
        in_author_block = True
        
        for i in range(author_info_idx + 1, len(lines)):
            line = lines[i].strip()
            
            # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            if (line.startswith('Â©') or 
                line.startswith('Copyright') or 
                line.startswith('DOI:') or 
                line.startswith('PMID:') or
                line.startswith('PMCID:') or
                line.startswith('Conflict of interest')):
                break
            
            if in_author_block:
                # Author ì •ë³´ ë¸”ë¡ ê±´ë„ˆë›°ê¸°
                if (line.startswith('(') and ')' in line) or '@' in line or '.edu' in line or '.org' in line:
                    continue
                elif not line:
                    in_author_block = False
                    continue
                elif any(keyword in line.upper() for keyword in ['INTRODUCTION:', 'BACKGROUND:', 'OBJECTIVE:', 'PURPOSE:', 'METHODS:']):
                    in_author_block = False
                    abstract_started = True
                    abstract_lines.append(line)
                else:
                    continue
            else:
                if not abstract_started and line:
                    abstract_started = True
                    abstract_lines.append(line)
                elif abstract_started and line:
                    abstract_lines.append(line)
        
        abstract = ' '.join(abstract_lines).strip() if abstract_lines else None
    
    # 4. ë°±ì—… ë°©ë²•: INTRODUCTION ë“± í‚¤ì›Œë“œë¡œ abstract ì°¾ê¸°
    if not abstract:
        abstract_keywords = ['INTRODUCTION:', 'BACKGROUND:', 'IMPORTANCE:', 'PURPOSE:', 'OBJECTIVE:']
        
        for keyword in abstract_keywords:
            pattern = f'\\n{re.escape(keyword)}|^{re.escape(keyword)}'
            match = re.search(pattern, record_text, re.IGNORECASE | re.MULTILINE)
            if match:
                start_pos = match.start()
                if record_text[start_pos] == '\\n':
                    start_pos += 1
                
                # ì¢…ë£Œ ìœ„ì¹˜ ì°¾ê¸°
                end_markers = ['Â©', 'Copyright', 'DOI:', 'PMID:', 'PMCID:', 'Conflict of interest']
                end_pos = len(record_text)
                
                for marker in end_markers:
                    marker_match = re.search(re.escape(marker), record_text[start_pos:], re.IGNORECASE)
                    if marker_match:
                        end_pos = start_pos + marker_match.start()
                        break
                
                raw_abstract = record_text[start_pos:end_pos].strip()
                abstract = re.sub(r'\s+', ' ', raw_abstract).strip()
                break
    
    return {
        'number': number,
        'title': title,
        'abstract': abstract,
        'doi': doi
    }

def load_abstracts_from_text(text_file_path):
    """abstract-depression-set.txtì—ì„œ ëª¨ë“  abstract ë¡œë“œ"""
    try:
        with open(text_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"Error reading {text_file_path}: {e}")
        return {}
    
    print("ğŸ“– abstract-depression-set.txtì—ì„œ ë°ì´í„° íŒŒì‹± ì¤‘...")
    
    # V2 íŒŒì„œì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ë ˆì½”ë“œ ë¶„í• 
    record_positions = []
    for match in re.finditer(r'^(\d+)\.\s+', content, re.MULTILINE):
        number = int(match.group(1))
        start_pos = match.start()
        record_positions.append((start_pos, number))
    
    # ìˆœì°¨ì  ë²ˆí˜¸ë§Œ í•„í„°ë§
    sequential_positions = []
    expected_num = 1
    
    for pos, num in record_positions:
        if num == expected_num:
            sequential_positions.append((pos, num))
            expected_num += 1
    
    print(f"âœ… {len(sequential_positions)}ê°œì˜ ìˆœì°¨ì  ë ˆì½”ë“œ ë°œê²¬")
    
    # ê° ë ˆì½”ë“œ íŒŒì‹±
    abstracts_dict = {}
    total_records = len(sequential_positions)
    
    for i, (start_pos, number) in enumerate(sequential_positions):
        # ì§„í–‰ ìƒí™© í‘œì‹œ (100ê°œë§ˆë‹¤)
        if (i + 1) % 100 == 0 or i == 0:
            progress = ((i + 1) / total_records) * 100
            print(f"  ğŸ“Š Abstract íŒŒì‹± ì§„í–‰: {i + 1}/{total_records} ({progress:.1f}%)")
        
        try:
            # ë ˆì½”ë“œ ë ìœ„ì¹˜ ê²°ì •
            if i < len(sequential_positions) - 1:
                end_pos = sequential_positions[i + 1][0]
            else:
                end_pos = len(content)
            
            record_text = content[start_pos:end_pos].strip()
            
            # íŒŒì‹± ì‹¤í–‰
            result = parse_single_record_v2(record_text, number)
            if result and result['title']:
                abstracts_dict[normalize_title(result['title'])] = result
                
        except Exception as e:
            print(f"Error parsing record {number}: {e}")
            continue
    
    print(f"ğŸ“š {len(abstracts_dict)}ê°œì˜ ì œëª©-ì¶”ìƒ ë§¤í•‘ ìƒì„±")
    return abstracts_dict

def load_meta_data(csv_file_path):
    """meta_article_data.csv ë¡œë“œ"""
    try:
        with open(csv_file_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            return list(reader)
    except Exception as e:
        print(f"Error reading {csv_file_path}: {e}")
        return []

def match_titles_and_extract(meta_data, abstracts_dict, similarity_threshold=0.7):
    """ì œëª© ë§¤ì¹­ ë° Abstract ì¶”ì¶œ"""
    
    print(f"\\nğŸ” ì œëª© ë§¤ì¹­ ì‹œì‘ (ì„ê³„ê°’: {similarity_threshold})")
    
    matches = {
        'exact': [],
        'fuzzy': [],
        'no_match': []
    }
    
    existing_abstracts = 0
    new_abstracts = 0
    
    total_rows = len(meta_data)
    
    for i, row in enumerate(meta_data):
        # ì§„í–‰ ìƒí™© í‘œì‹œ (100ê°œë§ˆë‹¤)
        if (i + 1) % 100 == 0 or i == 0:
            progress = ((i + 1) / total_rows) * 100
            print(f"  ğŸ“Š ì§„í–‰ ìƒí™©: {i + 1}/{total_rows} ({progress:.1f}%)")
        
        meta_title = row.get('Title', '').strip()
        existing_abstract = row.get('Abstract', '').strip()
        
        if existing_abstract:
            existing_abstracts += 1
            continue
        
        if not meta_title:
            matches['no_match'].append({'row': row, 'reason': 'Empty title'})
            continue
        
        normalized_meta_title = normalize_title(meta_title)
        
        # 1. ì •í™•í•œ ë§¤ì¹­ ì‹œë„
        if normalized_meta_title in abstracts_dict:
            matched_data = abstracts_dict[normalized_meta_title]
            row['Abstract'] = matched_data['abstract']
            if not row.get('DOI') and matched_data.get('doi'):
                row['DOI'] = matched_data['doi']
            
            matches['exact'].append({
                'meta_title': meta_title,
                'matched_title': matched_data['title'],
                'similarity': 1.0
            })
            new_abstracts += 1
            continue
        
        # 2. í¼ì§€ ë§¤ì¹­ ì‹œë„
        best_match = None
        best_similarity = 0
        
        for abstract_title_norm, abstract_data in abstracts_dict.items():
            similarity = calculate_similarity(normalized_meta_title, abstract_title_norm)
            
            if similarity > best_similarity and similarity >= similarity_threshold:
                best_similarity = similarity
                best_match = abstract_data
        
        if best_match:
            row['Abstract'] = best_match['abstract']
            if not row.get('DOI') and best_match.get('doi'):
                row['DOI'] = best_match['doi']
            
            matches['fuzzy'].append({
                'meta_title': meta_title,
                'matched_title': best_match['title'],
                'similarity': best_similarity
            })
            new_abstracts += 1
        else:
            # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œì—ë„ ê°€ì¥ ìœ ì‚¬í•œ ì œëª© ê¸°ë¡ (ê²€í† ìš©)
            closest_match = None
            closest_similarity = 0
            
            # ê°€ì¥ ìœ ì‚¬í•œ ì œëª© ì°¾ê¸° (ì„ê³„ê°’ ë¯¸ë§Œì´ì§€ë§Œ)
            for abstract_title_norm, abstract_data in abstracts_dict.items():
                similarity = calculate_similarity(normalized_meta_title, abstract_title_norm)
                if similarity > closest_similarity:
                    closest_similarity = similarity
                    closest_match = abstract_data
            
            matches['no_match'].append({
                'row': row, 
                'reason': f'No match above {similarity_threshold}',
                'closest_match_title': closest_match['title'] if closest_match else '',
                'closest_similarity': closest_similarity
            })
    
    # í†µê³„ ì¶œë ¥
    print(f"\\nğŸ“Š === ë§¤ì¹­ ê²°ê³¼ í†µê³„ ===")
    print(f"ğŸ“ ì „ì²´ ë©”íƒ€ ë°ì´í„°: {len(meta_data)}")
    print(f"ğŸ“ˆ ê¸°ì¡´ Abstract: {existing_abstracts}")
    print(f"âœ… ì •í™•í•œ ë§¤ì¹­: {len(matches['exact'])}")
    print(f"ğŸ” í¼ì§€ ë§¤ì¹­: {len(matches['fuzzy'])}")
    print(f"âŒ ë§¤ì¹­ ì‹¤íŒ¨: {len(matches['no_match'])}")
    print(f"ğŸ†• ìƒˆë¡œ ì¶”ê°€ëœ Abstract: {new_abstracts}")
    print(f"ğŸ“Š ìµœì¢… Abstract ìˆ˜: {existing_abstracts + new_abstracts}")
    print(f"ğŸ“ˆ ìµœì¢… Abstract ë¹„ìœ¨: {(existing_abstracts + new_abstracts)/len(meta_data)*100:.1f}%")
    
    return matches

def save_results(meta_data, matches, output_dir):
    """ê²°ê³¼ ì €ì¥"""
    
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # 1. í†µí•©ëœ ê²°ê³¼ CSV íŒŒì¼ ìƒì„± (ë§¤ì¹­ ì •ë³´ í¬í•¨)
    output_file = os.path.join(output_dir, f'title_matched_complete_{timestamp}.csv')
    
    if meta_data:
        # ê¸°ë³¸ í•„ë“œì— ë§¤ì¹­ ì •ë³´ ì¶”ê°€
        base_fieldnames = list(meta_data[0].keys())
        extended_fieldnames = base_fieldnames + ['Matched_Title', 'Match_Type', 'Match_Similarity', 'Match_Status']
        
        # ë§¤ì¹­ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        exact_matches = {match['meta_title']: match for match in matches['exact']}
        fuzzy_matches = {match['meta_title']: match for match in matches['fuzzy']}
        no_matches = {item['row'].get('Title', ''): item for item in matches['no_match'] if 'row' in item}
        
        try:
            with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=extended_fieldnames, quoting=csv.QUOTE_ALL)
                writer.writeheader()
                
                for row in meta_data:
                    title = row.get('Title', '')
                    
                    # ì›ë³¸ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ê³  ë§¤ì¹­ ì •ë³´ ì¶”ê°€
                    output_row = row.copy()
                    
                    # ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬
                    for key, value in output_row.items():
                        if isinstance(value, str):
                            # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ í•„ë“œ ì²´í¬ (10000ì ì´ìƒ)
                            if len(value) > 10000:
                                print(f"âš ï¸  ê²½ê³ : í–‰ {meta_data.index(row)+1}, í•„ë“œ '{key}'ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ê¸º ({len(value)}ì)")
                                # ì²« 5000ìë§Œ ìœ ì§€
                                output_row[key] = value[:5000] + "... [TRUNCATED]"
                            
                            # ê°œí–‰ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
                            output_row[key] = value.replace('\n', ' ').replace('\r', ' ')
                    
                    if title in exact_matches:
                        output_row['Match_Type'] = 'Exact'
                        output_row['Match_Similarity'] = exact_matches[title]['similarity']
                        output_row['Match_Status'] = 'Success'
                        output_row['Matched_Title'] = exact_matches[title]['matched_title']
                    elif title in fuzzy_matches:
                        output_row['Match_Type'] = 'Fuzzy'
                        output_row['Match_Similarity'] = fuzzy_matches[title]['similarity']
                        output_row['Match_Status'] = 'Success'
                        output_row['Matched_Title'] = fuzzy_matches[title]['matched_title']
                    elif title in no_matches:
                        output_row['Match_Type'] = 'None'
                        output_row['Match_Similarity'] = 0.0
                        output_row['Match_Status'] = f"Failed: {no_matches[title]['reason']}"
                        output_row['Matched_Title'] = ''
                    else:
                        # ê¸°ì¡´ì— Abstractê°€ ìˆì—ˆë˜ ê²½ìš°
                        output_row['Match_Type'] = 'Existing'
                        output_row['Match_Similarity'] = 1.0
                        output_row['Match_Status'] = 'Had Abstract'
                        output_row['Matched_Title'] = ''
                    
                    writer.writerow(output_row)
            
            print(f"\\nğŸ’¾ í†µí•©ëœ ê²°ê³¼ ì €ì¥: {output_file}")
        except Exception as e:
            print(f"Error saving complete data: {e}")
    
    # 2. ì‹¤íŒ¨í•œ í•­ëª©ë“¤ë§Œ ë³„ë„ CSVë¡œ ì €ì¥ (ê²€í† ìš©)
    failed_file = os.path.join(output_dir, f'failed_matches_{timestamp}.csv')
    
    try:
        with open(failed_file, 'w', newline='', encoding='utf-8-sig') as f:
            if matches['no_match']:
                # ì‹¤íŒ¨í•œ í•­ëª©ì˜ ì „ì²´ ì •ë³´ + ê°€ì¥ ìœ ì‚¬í•œ ì œëª© ì €ì¥
                failed_fieldnames = list(meta_data[0].keys()) + ['Failure_Reason', 'Closest_Match_Title', 'Closest_Similarity']
                writer = csv.DictWriter(f, fieldnames=failed_fieldnames, quoting=csv.QUOTE_ALL)
                writer.writeheader()
                
                for item in matches['no_match']:
                    if 'row' in item:
                        failed_row = item['row'].copy()
                        
                        # ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬
                        for key, value in failed_row.items():
                            if isinstance(value, str):
                                # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ í•„ë“œ ì²´í¬
                                if len(value) > 10000:
                                    failed_row[key] = value[:5000] + "... [TRUNCATED]"
                                # ê°œí–‰ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
                                failed_row[key] = value.replace('\n', ' ').replace('\r', ' ')
                        
                        failed_row['Failure_Reason'] = item['reason']
                        failed_row['Closest_Match_Title'] = item.get('closest_match_title', '')
                        failed_row['Closest_Similarity'] = item.get('closest_similarity', 0.0)
                        writer.writerow(failed_row)
        
        print(f"ğŸ’¾ ì‹¤íŒ¨ í•­ëª© CSV ì €ì¥: {failed_file} ({len(matches['no_match'])}ê°œ)")
    except Exception as e:
        print(f"Error saving failed matches: {e}")
    
    return output_file, failed_file

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("=== ì œëª© ê¸°ë°˜ Abstract ì¶”ì¶œ ì‹œì‘ ===")
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    script_dir = os.path.dirname(os.path.abspath(__file__))
    meta_csv_path = os.path.join(script_dir, '../data/meta_article_empty_abstract_data.csv')
    abstracts_txt_path = os.path.join(script_dir, 'abstract-depression-set.txt')
    output_dir = os.path.join(script_dir, 'output')
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(meta_csv_path):
        print(f"Error: {meta_csv_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if not os.path.exists(abstracts_txt_path):
        print(f"Error: {abstracts_txt_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\\nğŸ“ ë°ì´í„° ë¡œë”©...")
    meta_data = load_meta_data(meta_csv_path)
    if not meta_data:
        print("ë©”íƒ€ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
        return
    print(f"âœ… ë©”íƒ€ ë°ì´í„°: {len(meta_data)} rows")
    
    abstracts_dict = load_abstracts_from_text(abstracts_txt_path)
    if not abstracts_dict:
        print("Abstract ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
        return
    
    # 2. ì œëª© ë§¤ì¹­ ë° ì¶”ì¶œ
    matches = match_titles_and_extract(meta_data, abstracts_dict, similarity_threshold=0.7)
    
    # 3. ê²°ê³¼ ì €ì¥
    complete_file, failed_file = save_results(meta_data, matches, output_dir)
    
    print("\\nâœ… ì œëª© ê¸°ë°˜ Abstract ì¶”ì¶œ ì™„ë£Œ!")
    print(f"ğŸ“„ í†µí•© íŒŒì¼: {complete_file}")
    print(f"ğŸ“„ ì‹¤íŒ¨ í•­ëª©: {failed_file}")

if __name__ == "__main__":
    main()